# A/B Testing Portfolio: From Simulation to Real Impact
Welcome to the A/B Testing portfolio.
This repository documents my journey from learning the fundamentals through simulation to applying those skills on real datasets that drive business insights.
In other words, it's about mastering the logic behind it before analyzing its real-world complexity.
I've learned that A/B testing is one of the most essential tools in data science, whether for optimizing user experience, increasing revenue, or validating product decisions.
This repo shows how I approach experimentation with clarity, rigor, and reproducibility. It's not just statistical significance I was looking for, but practical significance and real impact.
<br></br>

### 1. Simulation: Learning the Core Concepts
Before touching real data, I built a fully controlled A/B test simulation to understand:

* How random assignment works

* How engagement metrics bevave

* How to compute confidence intervals, and p‑values

* How sample size affects statistical power

* How to interpret results for business stakeholders
<br></br>

This simulation allowed me to practice the entire workflow, which included:

* Generating synthetic users

* Assigning them to control/variant

* Simulating different A/B test metrics

* Running statistical tests

* Visualizing distributions
<br></br>

### 2. Real A/B Test: Revenue‑Based Experiment
After building confidence with simulation, I moved on to a real dataset containing:

* `USER_ID`
* `GROUP` (A vs. B)
* `PAGE_VIEWS`
* `TIME_SPENT`
* `CONVERSION` (YES/NO)
* `DEVICE` (MOBILE/DESKTOP)
* `LOCATION`
<br></br>

Key questions explored:
* Does the variant increase average page views and average time-on-page?

* Are engagement differences statistically significant?

* Do device type or location influence results?

* What version should the company adopt based on the evidence?

